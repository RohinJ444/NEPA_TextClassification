{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b492907",
   "metadata": {},
   "source": [
    "# Install necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyPDF2\n",
    "%pip install pytesseract\n",
    "%pip install pdf2image\n",
    "%pip install pdfplumber\n",
    "%pip install python-imaging\n",
    "%pip install langchain openai pypdf faiss-cpu\n",
    "%pip install python-docx\n",
    "%pip install exceptions\n",
    "%pip install pytesseract pdf2image python-docx PyPDF2 pyth\n",
    "%pip install striprtf\n",
    "%pip install tiktoken\n",
    "%pip install pinecone-client\n",
    "%pip install docx\n",
    "%pip install pyth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd621078",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2efc288",
   "metadata": {},
   "source": [
    "# Set up relevant functions for Text Vectorization and GPT-4 Querying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44a9c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import base64\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import pytesseract\n",
    "import pdf2image\n",
    "import docx\n",
    "import pinecone\n",
    "import time\n",
    "import tiktoken\n",
    "import uuid\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "# Extracts text from DOCX files\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "# Extracts text from RTF files\n",
    "def read_rtf(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        doc = Rtf15Reader.read(file)\n",
    "    return PlaintextWriter.write(doc).getvalue()\n",
    "\n",
    "# Function to read text from PDF of scanned documents using Optical Character Recognition (OCR)\n",
    "def ocr_pdf(file_path):\n",
    "    \n",
    "    # Convert each page to an image\n",
    "    images = pdf2image.convert_from_path(file_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for image in images:\n",
    "        # Use Tesseract to do OCR on the image\n",
    "        text += pytesseract.image_to_string(image)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to read text from text-encoded PDF\n",
    "def read_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Get page count of a PDF\n",
    "def get_pdf_page_count(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            return len(pdf_reader.pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Efficient function for processing any PDF. Attempts to transcribe encoded text first and uses OCR if that fails\n",
    "def process_pdf(file_path):\n",
    "    try:\n",
    "        output = read_pdf(file_path)\n",
    "        \n",
    "        # If less than 30 words are extracted per page on average, assume failure and use OCR\n",
    "        if (len(output) < (get_pdf_page_count(file_path) * 30 * 7)):\n",
    "            output = ocr_pdf(file_path)\n",
    "            \n",
    "        return output\n",
    "    except:\n",
    "        return ocr_pdf(file_path)\n",
    "    \n",
    "# Function to read and combine all files in a single folder\n",
    "def concatenate_pdfs_in_folder(folder_path):\n",
    "    text = \"\"\n",
    "    for file_path in glob.glob(os.path.join(folder_path, '*')):\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            text += process_pdf(file_path) + \"\\n\\n\\n NEW FILE BEGINS \\n\\n\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to make a simple text-based query to the OpenAI API\n",
    "def text_query_gpt(definitions, document_text):\n",
    "    \n",
    "    prompt_text = f\"The following are definitions of carefully identified exogenous variables for projects that undergo NEPA Review, please read them all very carefully and become deeply and intimately familiar with every detail:\\n\\n\" + f\"{definitions}\\n\\n\" + f\"Read the following NEPA Environmental Impact Statements (these may include draft and/or final environmental impact statements) and related comment letters:\\n\\n\" + f\"\\n\\n {document_text} \\n\\n\" + f\"Based on the definitions above, indicate the presence (1) or absence (0) of each exogenous variable in the highlighted portions of the EIS extract and related documents. Please, be meticulous in your search and systematic in your logic, methods, and justifications for the presence of each variable (listed again here for clarity: [Lack of Federal Funding/Strained Resources for NEPA Compliance Teams, Influence of NEPA Litigation, Property/Land Rights Disputes, Mid-project financing issues/loss of project funding, Compliance with ESA triggered during NEPA Review Process, Compliance with CWA triggered during NEPA Review Process, Compliance with CAA triggered during NEPA Review Process, Compliance with NHPA triggered during NEPA Review Process, Third-party construction delays, Compliance with other environmental legislation triggered during NEPA Review Process]), making sure to tie them back closely to the provided definitions. Remember that some of the definitions are broad, particularly for strained agency resources and for mid-project financing issues, so your search should be of great breadth and depth. YOU MUST FORMAT YOUR OUTPUT EXACTLY AS FOLLOWS: 'Lack of Federal Funding/Strained Resources for NEPA Compliance Teams: [insert 0 or 1 here], Influence of NEPA Litigation: [insert 0 or 1 here], Property/Land Rights Disputes: [insert 0 or 1 here], Mid-project financing issues/loss of project funding: [insert 0 or 1 here], Compliance with ESA triggered during NEPA Review Process: [insert 0 or 1 here], Compliance with CWA triggered during NEPA Review Process: [insert 0 or 1 here], Compliance with CAA triggered during NEPA Review Process: [insert 0 or 1 here], Compliance with NHPA triggered during NEPA Review Process: [insert 0 or 1 here], Third-party construction delays: [insert 0 or 1 here], Compliance with other environmental legislation triggered during NEPA Review Process: [insert 0 or 1 here]'. DO NOT INCLUDE ANY ADDITIONAL TEXT IN YOUR OUTPUT. BE PRECISE WITH YOUR METHODOLOGY, IT SHOULD BE CONSISTENT FOR REPEATED QUERIES. If no relevant material to the exogenous variable definition appears in the provided excerpts, you can classify them as 0, but feel free to make context-based inferences. Do read all provided material meticulously\"\n",
    "    \n",
    "    client = OpenAI(api_key = 'REDACTED')\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Your sole duty is to carefully parse environmental permitting documents with a precise eye for detail, and report back your findings per the user's desired specifications.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Gets token length of text\n",
    "def tiktoken_len(text):\n",
    "    tokenizer_name = tiktoken.encoding_for_model('gpt-4')\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_name.name)\n",
    "    \n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "# Function that breaks down very large EISes/text strings (128000+ tokens) into smaller tokenized vector chunks passable to OpenAI API via embeddings\n",
    "def text_vectorizer(document_text, max_chunk_length):\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_length,\n",
    "        chunk_overlap=20,\n",
    "        length_function=tiktoken_len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "        \n",
    "    chunks = []\n",
    "    split_texts = text_splitter.split_text(document_text)\n",
    "\n",
    "    for i, chunk_text in enumerate(split_texts):\n",
    "        chunks.append({\n",
    "            'id': str(uuid.uuid4()),  # Generate a unique identifier for each chunk\n",
    "            'text': chunk_text,\n",
    "            'chunk': i  # Chunk number\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "# Function that creates embeddings for tokenized vector chunks via Langchain with server-side storage on Pinecone for access and use by OpenAI's GPT-4 model\n",
    "# Extracts top 300 vector chunks ranked by relevance to prompt via cosine function, sorts them by keyword search, and then uses top 125 for processing\n",
    "def vectorized_query_gpt(definitions, vector_chunks):\n",
    "    \n",
    "    client = OpenAI(api_key = 'REDACTED')\n",
    "\n",
    "    openai.api_key = 'REDACTED'\n",
    "    embed_model = \"text-embedding-3-large\"\n",
    "\n",
    "    res = client.embeddings.create(\n",
    "        input=[\n",
    "            \"Sample document text goes here\",\n",
    "            \"there will be several phrases in each batch\"\n",
    "        ], model=embed_model\n",
    "    )\n",
    "\n",
    "    # configure Pinecone client\n",
    "    pc = Pinecone(api_key='REDACTED')\n",
    "    \n",
    "    cloud = 'aws'\n",
    "    region = 'us-east-1'\n",
    "    index_name = 'gpt-4-langchain-docs'\n",
    "\n",
    "    spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "    \n",
    "    # check if index already exists (it shouldn't if this is first time)\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        # if does not exist, create index\n",
    "        pc.create_index(\n",
    "            index_name,\n",
    "            dimension=len(res.data[0].embedding),\n",
    "            metric='cosine',\n",
    "            spec=spec\n",
    "        )\n",
    "        # wait for index to be initialized\n",
    "        while not pc.describe_index(index_name).status['ready']:\n",
    "            time.sleep(1)\n",
    "\n",
    "    # connect to index\n",
    "    index = pc.Index(index_name)\n",
    "    \n",
    "    batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "    for i in tqdm(range(0, len(vector_chunks), batch_size)):\n",
    "        i_end = min(len(vector_chunks), i+batch_size)\n",
    "        meta_batch = vector_chunks[i:i_end]\n",
    "        \n",
    "        ids_batch = [x['id'] for x in meta_batch]\n",
    "        \n",
    "        texts = [x['text'] for x in meta_batch]\n",
    "        \n",
    "        # create embeddings (try-except added to avoid RateLimitError)\n",
    "        try:\n",
    "            res = client.embeddings.create(input=texts, model=embed_model)\n",
    "        except:\n",
    "            done = False\n",
    "            while not done:\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "                    done = True\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "        embeds = [datum.embedding for datum in res.data]\n",
    "        \n",
    "        # cleanup metadata\n",
    "        meta_batch = [{\n",
    "            'id': x['id'],\n",
    "            'text': x['text'],\n",
    "            'chunk': x['chunk'],\n",
    "        } for x in meta_batch]\n",
    "        \n",
    "        \n",
    "        to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "        \n",
    "        # upsert to Pinecone\n",
    "        index.upsert(vectors=to_upsert)\n",
    "\n",
    "    query = f\"The following are definitions of carefully identified exogenous variables for projects that undergo NEPA Review, please read them all very carefully and become deeply and intimately familiar with every detail:\\n\\n\" + f\"{definitions}\\n\\n\" + f\"Read the following NEPA Environmental Impact Statements (these may include draft and/or final environmental impact statements) and related comment letters:\\n\\n\" + f\"Based on the definitions above, indicate the presence (1) or absence (0) of each exogenous variable in the highlighted portions of the EIS extract and related documents. Please, be meticulous in your search and systematic in your logic, methods, and justifications for the presence of each variable (listed again here for clarity: [Lack of Federal Funding/Strained Resources for NEPA Compliance Teams, Influence of NEPA Litigation, Property/Land Rights Disputes, Mid-project financing issues/loss of project funding, Compliance with ESA triggered during NEPA Review Process, Compliance with CWA triggered during NEPA Review Process, Compliance with CAA triggered during NEPA Review Process, Compliance with NHPA triggered during NEPA Review Process, Third-party construction delays, Compliance with other environmental legislation triggered during NEPA Review Process]), making sure to tie them back closely to the provided definitions. Remember that some of the definitions are broad, especially for strained agency resources and mid-project financing issues, so your search should be of great breadth and depth. YOU MUST FORMAT YOUR OUTPUT EXACTLY AS FOLLOWS: 'Lack of Federal Funding/Strained Resources for NEPA Compliance Teams: [insert 0 or 1 here], Influence of NEPA Litigation: [insert 0 or 1 here], Property/Land Rights Disputes: [insert 0 or 1 here], Mid-project financing issues/loss of project funding: [insert 0 or 1 here], Compliance with ESA triggered during NEPA Review Process: [insert 0 or 1 here], Compliance with CWA triggered during NEPA Review Process: [insert 0 or 1 here], Compliance with CAA triggered during NEPA Review Process: [insert 0 or 1 here], Compliance with NHPA triggered during NEPA Review Process: [insert 0 or 1 here], Third-party construction delays: [insert 0 or 1 here], Compliance with other environmental legislation triggered during NEPA Review Process: [insert 0 or 1 here]'. DO NOT INCLUDE ANY ADDITIONAL TEXT IN YOUR OUTPUT. BE PRECISE WITH YOUR METHODOLOGY, IT SHOULD BE CONSISTENT FOR REPEATED QUERIES. If no relevant material to the exogenous variable definition appears in the provided excerpts, you can classify them as 0, but feel free to make context-based inferences. Do read all provided material meticulously\"\n",
    "\n",
    "    res = client.embeddings.create(\n",
    "        input=[query],\n",
    "        model=embed_model\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res.data[0].embedding\n",
    "\n",
    "    # get relevant contexts via vector search (primary)\n",
    "    res = index.query(vector=xq, top_k=300, include_metadata=True)\n",
    "\n",
    "    # Keyword scoring (secondary)\n",
    "    def keyword_score(text, keywords):\n",
    "        score = sum(text.lower().count(kw.lower()) for kw in keywords)\n",
    "        return score\n",
    "    \n",
    "    keywords = [\"contractor\", \"timeline\", \"approval\", \"delay\", \"lawsuit\", \"external\", \"year\", \"construction\", \"developer\", \"finance\", \"owner\", \"litigat\", \"Clean Water Act\", \"endangered species\", \"Clean Air Act\", \"historic\", \"historic preservation\", \"landmark\", \"protected\", \"financ\", \"budget\", \"fund\", \"setback\", \"extend\", \"lengthen\", \"public comment\", \"comment\", \"stakeholder\", \"plaintiff\", \"defendant\", \"employment\", \"staffing\", \"shortfall\", \"resource\", \"strained\", \"allocation\", \"legal\", \"challenge\", \"date\", \"final\", \"staffing\"]\n",
    "\n",
    "    # Re-rank based on keywords\n",
    "    matches_with_scores = [(item, keyword_score(item['metadata']['text'], keywords)) for item in res['matches']]\n",
    "    matches_with_scores.sort(key=lambda x: x[1], reverse=True)  # Sort by keyword score, high to low\n",
    "\n",
    "    top_matches = matches_with_scores[:125]  # Select the top 125 after re-ranking\n",
    "\n",
    "    contexts = [item[0]['metadata']['text'] for item in top_matches]\n",
    "    augmented_query = query+\"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"\n",
    "    \n",
    "    primer = \"Your sole duty is to carefully parse environmental permitting documents with a precise eye for detail, and report back your findings per the user's desired specifications.\"\n",
    "\n",
    "    # print(tiktoken_len(augmented_query))\n",
    "    \n",
    "    res = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": primer},\n",
    "            {\"role\": \"user\", \"content\": augmented_query}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return res.choices[0].message.content\n",
    "\n",
    "def add_output_to_csv(project_name, model_output, dataframe):\n",
    "    exo_variables = [\"Lack of Federal Funding/Strained Resources for NEPA Compliance Teams\", \"Influence of NEPA Litigation\", \"Property/Land Rights Disputes\", \"Mid-project financing issues/loss of project funding\", \"Compliance with ESA triggered during NEPA Review Process\", \"Compliance with CWA triggered during NEPA Review Process\", \"Compliance with CAA triggered during NEPA Review Process\", \"Compliance with NHPA triggered during NEPA Review Process\", \"Third-party construction delays\", \"Compliance with other environmental legislation triggered during NEPA Review Process\"]\n",
    "    \n",
    "    print(model_output)\n",
    "    print()\n",
    "    \n",
    "    output_dict = {}\n",
    "    \n",
    "    try:\n",
    "        output_dict = {item.split(': ')[0].strip(): int(item.split(': ')[1]) for item in model_output.strip(\"'\").split(', ')}\n",
    "    except:\n",
    "        return f\"failure loading EISes for {project_name}\"\n",
    "        \n",
    "\n",
    "    row_index = 0\n",
    "    \n",
    "    try:\n",
    "        row_index = dataframe[dataframe['Project Title'] == project_name].index[0]\n",
    "    except:\n",
    "        return f\"Failure finding row in spreadsheet for {project_name}!\"\n",
    "\n",
    "    for var in exo_variables:\n",
    "        dataframe.loc[row_index, var] = output_dict[var]\n",
    "        \n",
    "    return f\"Success! for {project_name}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a139b",
   "metadata": {},
   "source": [
    "# Script to Run Systematic Queries and Update Spreadsheets with Exogenous Variable Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "csv_path = 'andres_dot.csv'\n",
    "\n",
    "spreadsheet = pd.read_csv(csv_path)\n",
    "spreadsheet['Project Title'] = spreadsheet['Project Title'].str.strip()\n",
    "\n",
    "# Read the exogenous variable definitions from the provided PDF\n",
    "definitions_pdf_path = 'Compiled Exogenous Variable Definitions.pdf'\n",
    "definitions_text = read_pdf(definitions_pdf_path)\n",
    "\n",
    "base_path = '/Users/rohinjuneja/Documents/NEPA_GPT/JIGNESH_GPT'\n",
    "subfolder_path = '/DOTPERMIT_ANDRES'\n",
    "base_path += subfolder_path\n",
    "print(base_path)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Iterate over each folder in the subfolder and process all PDFs within it\n",
    "for folder_name in os.listdir(base_path):\n",
    "    folder_path = os.path.join(base_path, folder_name)\n",
    "    folder_path = folder_path[folder_path.index(\"JIGNESH_GPT\") + len(\"JIGNESH_GPT \"):] \n",
    "\n",
    "    project_name = folder_path[folder_path.index(subfolder_path[1:]) + len(subfolder_path):]\n",
    "    print(project_name)\n",
    "    \n",
    "    if ('Plutonium' not in project_name) and spreadsheet.loc[spreadsheet['Project Title'] == project_name, 'Influence of NEPA Litigation'].isnull().any():\n",
    "    \n",
    "        input_text = concatenate_pdfs_in_folder(folder_path)\n",
    "        model_output = ''\n",
    "\n",
    "        if tiktoken_len(input_text) < 32000:\n",
    "            model_output = text_query_gpt(definitions_text, input_text)\n",
    "        else:    \n",
    "            model_output = vectorized_query_gpt(definitions_text, text_vectorizer(input_text, 500))\n",
    "\n",
    "        outcome = add_output_to_csv(project_name, model_output, spreadsheet)\n",
    "\n",
    "        print()\n",
    "        print(outcome)\n",
    "        print()\n",
    "\n",
    "        spreadsheet.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a3bab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lack of Federal Funding/Strained Resources for NEPA Compliance Teams: 0, Influence of NEPA Litigation: 0, Property/Land Rights Disputes: 0, Mid-project financing issues/loss of project funding: 0, Compliance with ESA triggered during NEPA Review Process: 1, Compliance with CWA triggered during NEPA Review Process: 1, Compliance with CAA triggered during NEPA Review Process: 1, Compliance with NHPA triggered during NEPA Review Process: 1, Third-party construction delays: 0, Compliance with other environmental legislation triggered during NEPA Review Process: 0\n",
      "{'Lack of Federal Funding/Strained Resources for NEPA Compliance Teams': 0, 'Influence of NEPA Litigation': 0, 'Property/Land Rights Disputes': 0, 'Mid-project financing issues/loss of project funding': 0, 'Compliance with ESA triggered during NEPA Review Process': 1, 'Compliance with CWA triggered during NEPA Review Process': 1, 'Compliance with CAA triggered during NEPA Review Process': 1, 'Compliance with NHPA triggered during NEPA Review Process': 1, 'Third-party construction delays': 0, 'Compliance with other environmental legislation triggered during NEPA Review Process': 0}\n"
     ]
    }
   ],
   "source": [
    "vector = vectorized_query_gpt(definitions_text, text_vectorizer(concatenate_pdfs_in_folder(\"DOI/Blackfoot Bridge Mine Project Developing Three Mine Pits Haul Roads Water Management Structures and Overburden Disposal Areas Implementation Caribou County ID\"), 500))\n",
    "print(vector)\n",
    "output_dict = {item.split(': ')[0].strip(): int(item.split(': ')[1]) for item in vector.strip(\"'\").split(', ')}\n",
    "print(output_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
